---
title: "Bivalves_Analysis"
author: "Dan Bez Golanski"
date: "`r Sys.Date()`"
output: html_document
---

##Functions

```{r}
# The function takes datetime data that was saved as text in csv/xlsx file and convert it to POSIxlt, can handle combined data including both numeric and non numeric datetime
Convert_time_csv <- function(data, final_format, initial_format="ymd HMS",correct_time=T ) {
  temp_data<-data
  #for data saved as numeric in csv
  temp_data<-as.numeric(temp_data)
  temp_data<-as_datetime(as_date(temp_data, origin = "1899-12-30"), tz = Sys.timezone())
  #for data not saved as numeric in csv
  na_index<-which(is.na(temp_data))
  if(length(na_index)!=length(data))
  {
    for (i in na_index) {
      temp_data[i]<-parse_date_time(data[i],orders = initial_format,truncated = 3)
    }
  } else # if all data is character
  {
    temp_data<-parse_date_time(data,orders = initial_format,truncated = 3)
  }
  temp_data <- as.POSIXlt(temp_data)
  if(correct_time)
  {
    #correct time that rounded wrongly because it was numeric
    for (row_num in 1:length(temp_data)) {
    temptime<-unclass(temp_data[row_num])
    tempsec<-temptime$sec
    if (tempsec>58) {
      temp_data[row_num]<-temp_data[row_num]+1
      }
    }
  }
  temp_data<-strftime(temp_data,format = final_format)
  return(temp_data)
}

# This functions gets a bivalve df and returns it after calculate the different metrics for a given time interval
df_cut_times <- function(biv_df,date_col_num,break_interval,date_format) {
  biv_df[,date_col_num] <- parse_date_time(biv_df[,date_col_num],orders = date_format)
  biv_df$group <- cut(biv_df[,date_col_num], breaks= break_interval)
  biv_df_cut_time <-biv_df %>%
  group_by(Deployment.number,Biv_num,group) %>%
  summarise(biv_open_interval_perc=mean(biv_open_percent),biv_open_interval=mean(biv_open),per99.9=mean(percentile.99))
  return(biv_df_cut_time)
}
```

##packages

```{r}
library(dplyr)
library(ggplot2)
library(lubridate)
library(openxlsx)
library(tidyr)
library(GGally)
library(scales)
library(Hmisc)
library(StreamMetabolism)

```

##load data

```{r}

db<-readRDS("Big_Hobo_database_raw.RDS")
db2wide<-read.csv("Big_Hobo_database_wide_2min_percent.csv")
dblong<-read.csv("Big_Hobo_databse_long_raw.csv")
dblong2<-read.csv("Big_Hobo_databse_long_2min_percent.csv")
dblong10<-read.csv("Big_Hobo_databse_long_10min_percent&env.csv")
dblong_hour <- read.csv("Big_Hobo_databse_long_hour_percent.csv")
```

##Change raw data to long format

```{r}
col_names<-c("Bivalve.4","Bivalve.6","Bivalve.7","Bivalve.8","Bivalve.0","Bivalve.12","Bivalve.21...","Bivalve.19","Bivavle.15","Bivavle.18","Bivavle.23","Bivavle.22")
dblong<-gather_(db,"Biv_num","biv_open",col_names,factor_key = T,na.rm = T)
dblong$Date...Time <- parse_date_time(dblong$Date...Time,orders = "dmy HMS",truncated = 3)
write.csv(dblong,"Big_Hobo_databse_long_raw.csv")
```

##Calculate opening percentages

```{r}
max_deploy<-max(dblong$Deployment.number)
for (dep_num in 1:max_deploy) {
  temp_array_deploy<-dblong[dblong$Deployment.number==dep_num,]
  biv_num<-unique(temp_array_deploy$Biv_num)
  for (biv_index in 1:length(biv_num)) {
    temp_array_bivnum <- temp_array_deploy %>%  filter(Biv_num==biv_num[biv_index])
    per99<-quantile(temp_array_bivnum$biv_open,0.999)
    dblong[dblong$Deployment.number==dep_num&dblong$Biv_num==biv_num[biv_index],5]<-per99
    names(dblong)[5]<-"percentile.99"
  }
}
rm(temp_array_bivnum,temp_array_deploy)
dblong$biv_open_percent<-dblong$biv_open/dblong$percentile.99*100
write.csv(dblong,"Big_Hobo_databse_long_raw_percent.csv")
```

##Average by different time intervals

```{r}
dblong2 <- df_cut_times(dblong,1,"2 min","ymd HMS")
dblong10 <- df_cut_times(dblong,1,"10 min","ymd HMS")
dblong_hour <- df_cut_times(dblong,1,"1 hour","ymd HMS")
dblong_week <- df_cut_times(dblong,1,"1 week","ymd HMS")
write.csv(dblong2, "Big_Hobo_databse_long_2min_percent.csv")
write.csv(dblong10, "Big_Hobo_databse_long_10min_percent.csv")
write.csv(dblong_hour, "Big_Hobo_databse_long_hour_percent.csv")
write.csv(dblong_week, "Big_Hobo_databse_long_week_percent.csv")

```

## Env_10min

```{r}
mx<-readRDS("BigMx.RDS")
obs<-readRDS("BigOBS.RDS")
#mx
names(mx)<-c("group","Temp","lux")
mx$group <- Convert_time_csv(mx$group,"%Y-%m-%d %H:%M:%S")
write.xlsx(mx,"BigMx.xlsx")
#merge to the bigHoboDatabase
dblong10<-merge(dblong10,mx,by="group",all.x = T)

#obs
names(obs)<-c("group","Turbidity")
obs$group <- Convert_time_csv(obs$group,"%Y-%m-%d %H:%M:%S")
write.xlsx(obs,"BigOBS.xlsx")
#merge to the bigHoboDatabase
dblong10<-merge(dblong10,obs,by="group",all.x = T)

#change NA to none
dblong10<-arrange(dblong10,Deployment.number,Biv_num,group)
dblong10 <- replace(dblong10, is.na(dblong10), "")
write.csv(dblong10, "Big_Hobo_databse_long_10min_percent&env.csv")
```

## Env_1hour and add obs and mx 1 hour average

```{r}
waves<-readRDS("Waves_data.RDS")
currents<-readRDS("Currents_data.RDS")
# waves
names(waves)[1]<-"group"
waves$group <- Convert_time_csv(waves$group,final_format = "%Y-%m-%d %H:%M:%S",initial_format = "dmy HM",correct_time = F)
dblong_hour<-merge(dblong_hour,waves,by="group",all.x = T)
rm(waves)

#currents
#get only bottom currents
currents<-currents %>% filter(Depth..m.==23)
currents<-currents[,-2]
names(currents)[1]<-"group"
currents$group <- Convert_time_csv(currents$group,"%Y-%m-%d %H:%M:%S","dmy HM",correct_time = F)
dblong_hour<-merge(dblong_hour,currents,by="group",all.x = T)
rm(currents)

#change NA to none
names(dblong_hour)<-c("Date_time","Deployment.number","Bivalve.number","mean.biv.gap.precent","mean.biv.open.degree","percentile.99","Hs[m]","Tp[s]","Dp[deg]","H1/10[m]","Tmean[s]","Current.Speed[cm/sec]","Current.Direction[deg]")
dblong_hour<-arrange(dblong_hour,Deployment.number,Bivalve.number,Date_time)
dblong_hour <- replace(dblong_hour, is.na(dblong_hour), "")

#average mx and obs and merge them
env_10min_to_1hour <- dblong10
#getting only environmental data and remove duplicates
env_10min_to_1hour <- env_10min_to_1hour %>% 
  select(Temp,lux,Turbidity,group) %>% 
  distinct()
env_10min_to_1hour$group <- parse_date_time(env_10min_to_1hour$group,orders = "ymd HMS")
env_10min_to_1hour$Date_time <- cut(env_10min_to_1hour$group, breaks= "1 hour")
env_10min_to_1hour$Temp <- as.numeric(env_10min_to_1hour$Temp)
env_10min_to_1hour$lux <- as.numeric(env_10min_to_1hour$lux)
env_10min_to_1hour$Turbidity <- as.numeric(env_10min_to_1hour$Turbidity)
env_10min_to_1hour <-env_10min_to_1hour %>%
  group_by(Date_time) %>%
  summarise(Temp_1hour_mean=mean(Temp),Lux_1hour_mean=mean(lux),Turbidity_1hour_mean=mean(Turbidity))
dblong_hour<-merge(dblong_hour,env_10min_to_1hour,by="Date_time",all.x = T)
rm(env_10min_to_1hour)
write.csv(dblong_hour, "Big_Hobo_databse_long_hour_percent&env.csv")
```

## Env_1week - to complete!
```{r}
reco<-read.csv("RECO_Database_2022-2023.csv")
reco$mm.dd.yyyy<-parse_date_time(reco$mm.dd.yyyy,orders = "dmy",truncated = 3)
reco$group <- cut(reco$mm.dd.yyyy, breaks="week")
reco <- reco %>%filter(Station=="ST3")# %>%
  # select(group,Cal.Chl...ug.L.,Depth..m.) 
  
```

##2min sunrise.set and Day/Night calculations
```{r}
##Sunrise.set calculations

#sampling point coordinates
rei_point_crds <- c(32.4015190,34.8579670)
dblong2$group<- parse_date_time(dblong2$group,orders = "ymd HMS")
#creating column represent only the sampling dates
dblong2$Date <- as.Date(dblong2$group)
sampling_dates <- unique(dblong2$Date)
#create a data frame for sunrise.set for all the sampling dates
temp_date <- data.frame(matrix(NA,ncol=3,nrow=length(sampling_dates)))
names(temp_date) <- c("Date","Sunrise","Sunset")
temp_date$Date <- sampling_dates
rm(sampling_dates)
#calculate sunrise.set
for (day in 1:length(temp_date$Date)) {
  sunrise_set <- sunrise.set(rei_point_crds[1],rei_point_crds[2], as.Date(temp_date$Date[day]),timezone = "Asia/Jerusalem")
  temp_date$Sunrise[day] <- as.POSIXct(sunrise_set$sunrise)
  temp_date$Sunset[day] <- as.POSIXct(sunrise_set$sunset)
}
#change from numeric to datetime
temp_date$Sunrise <- as.POSIXct(as.numeric(temp_date$Sunrise),origin='1970-01-01')
temp_date$Sunset <- as.POSIXct(as.numeric(temp_date$Sunset),origin='1970-01-01')
temp_date$Sunrise <- force_tz(temp_date$Sunrise,tzone = "UTC") 
temp_date$Sunset <- force_tz(temp_date$Sunset,tzone = "UTC") 

dblong2<-merge(dblong2,temp_date,by="Date",all.x = T)
dblong2$Date <- NULL
rm(temp_date)


##Day.Night Calculations
dblong2$Day_Night <- ifelse(dblong2$group>dblong2$Sunrise & dblong2$group<dblong2$Sunset,"Day","Night")
#create new data frame composing only from distinct date times
temp_sun <- dblong2 %>% select(group,Sunrise,Sunset,Day_Night,Deployment.number) %>% distinct()
#calculate time difference from sunset for the day times
temp_sun$diff_sunset[temp_sun$Day_Night=="Day"] <- difftime(temp_sun$group[temp_sun$Day_Night=="Day"],temp_sun$Sunset[temp_sun$Day_Night=="Day"],units = "hours")
#calculate time difference for night times, complicated because the change of the dates at midnight
temp_night <- temp_sun[temp_sun$Day_Night=="Night",]
temp_night <- temp_night %>% arrange(group)
#calculate for times before midnight
temp_night$diff_sunset[as.numeric(format(temp_night$group,"%H"))>10] <-difftime(temp_night$group[ as.numeric(format(temp_night$group,"%H"))>10],temp_night$Sunset[as.numeric(format(temp_night$group ,"%H"))>10],units = "hours")

#calculate time difference for night times after midnight
dep_num <- 0
for (row_num in 1:dim(temp_night)[1]) {
  if (!is.na(temp_night$diff_sunset[row_num])) {
    #save the sunset time of the date before midnight
    temp_sunset <- temp_night$Sunset[row_num]
    #save the deployment number to avoid deployments mixing
    dep_num <- temp_night$Deployment.number[row_num]
  } else if (is.na(temp_night$diff_sunset[row_num]) & temp_night$Deployment.number[row_num]==dep_num) {
    temp_night$diff_sunset[row_num] <- difftime(temp_night$group[row_num],temp_sunset)
  }
}
#merging all the  data frames together
temp_night <- temp_night %>% select(group,diff_sunset)
#merging two partly full columns into one
temp_sun <- temp_sun %>% left_join(y,by="group") %>%
  mutate(diff_sunset = ifelse(is.na(diff_sunset.x),diff_sunset.y,diff_sunset.x)) %>% 
  select(-diff_sunset.x,-diff_sunset.y)
temp_sun <- temp_sun %>% select(group,diff_sunset) %>% arrange(group)
dblong2 <- merge(dblong2,temp_sun,by = "group",all.x = T)
rm(temp_sun)
rm(temp_night)
saveRDS(dblong2,"Big_Hobo_database_long_2min_percent&sun.RDS")
```


##Bivalves correlations
```{r}
#Bivalves correlations
dbwide2 <- dblong2 %>% select(group,Deployment.number,Biv_num,biv_open_interval_perc) %>% 
  spread(key = Biv_num,value = biv_open_interval_perc)
write.csv(dbwide2,"Big_Hobo_database_wide_2min_percent.csv")
deployments <- unique(dbwide2$Deployment.number)
bivalve_corr <- data.frame(dep_num=integer(),
                           corr_values=double(),
                           p_values=double(),
                           n=integer()
)
for (dep in deployments) {
temp_dep <- dbwide2 %>% filter(Deployment.number==dep)
#remove bivalves weren't deployed in the specific deployment
temp_dep <- temp_dep[,colSums(!is.na(temp_dep))>0]
# remove datetime and deployment number
temp_dep <- temp_dep[-1:-2]
#getting correlation, p-values and n and convert them to data frame
temp_corr <- rcorr(as.matrix(temp_dep),type = "pearson")
temp_corr <-data.frame(dep_num = paste("Deployment",dep),
                       corr_values = as.vector(temp_corr[["r"]][upper.tri(temp_corr[["r"]])]),
                       p_values = as.vector(temp_corr[["P"]][upper.tri(temp_corr[["P"]])]),
                       n = as.vector(temp_corr[["n"]][upper.tri(temp_corr[["n"]])])
                       )
bivalve_corr <- rbind(bivalve_corr,temp_corr)                       
}
rm(temp_corr,temp_dep)
write.csv(bivalve_corr,"Bivalves_correlations.csv")
hist(bivalve_corr$corr_values)

```


## Including Plots

```{r}
#seasonal and daily trend along bivalves - doesn't work properly!
dblong2$group <- parse_date_time(dblong2$group,orders = "ymd HMS")
dblong2$Month <- format(dblong2$group,"%Y-%m")
months_gaping_graph <- ggplot(dblong2,aes(x=group,y=biv_open_interval_perc))+
  geom_point(aes(color="gray89",size=0.1,alpha=0.5))+
  geom_smooth(se=T,size=2,color="blue")+
  scale_x_time(labels = time_format("%H:%M"),breaks = "4 hours")+
  scale_y_continuous(limits = c(0,100))+
  facet_wrap(~Month, scales = "free_x",ncol = length(unique(dblong2$Month))) +
  labs(x = "Time of Day",
       y = "Gaping Percent")
  months_gaping_graph
  
#Day Night difference - doesn't work!
  dblong2$Day_Night <- factor(dblong2$Day_Night)
Day_Night_gaping_graph <- ggplot(dblong2, aes(x = group, y = biv_open_interval_perc, color = Day_Night)) +
    geom_point(size = 0.1, alpha = 0.5, color = "gray") +
    geom_smooth(se = T, size = 1.5, aes(group = Day_Night), span = 0.2,fill="lightblue")+ 
    scale_x_time(labels = time_format("%H:%M"),breaks = "4 hours")+
    scale_y_continuous(limits = c(0,100))+
    labs(title = "Bivalve Open Interval Percentage Over Time",
         x = "Time",
         y = "Bivalve Open Interval Percentage",
         color = "Day/Night") 
Day_Night_gaping_graph

#bivalve correlation graph - not on top of each other and not beautiful!
correlation_hitsogram <- ggplot(bivalve_corr, aes(corr_values)) +
  geom_histogram(binwidth = 0.1,breaks = seq(-1, 1, by = 0.1), fill = "lightblue", color = "black") +
  scale_x_continuous(limits = c(-1,1))+
  labs(title = "Distribution of corr_values with Box Plot",
       x = "Correlation values",
       y = "Count") 

correlation_boxplot <- ggplot(bivalve_corr, aes(corr_values))+geom_boxplot(aes(y=corr_values,x=NULL))+
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", size = 1) +coord_flip()

layout(mat = matrix(c(1,2),2,1, byrow=TRUE),  height = c(1,8))
 
# Draw the boxplot and the histogram 
par(mar=c(0, 3.1, 1.1, 2.1))
boxplot(bivalve_corr$corr_values , horizontal=TRUE , ylim=c(-1,1), xaxt="n" , col=rgb(0.8,0.8,0,0.5) , frame=F)
par(mar=c(4, 3.1, 1.1, 2.1))
hist(bivalve_corr$corr_values , breaks=40 , col=rgb(0.2,0.8,0.5,0.5) , border=F , main="" , xlab="Bivalves correlations", xlim=c(-1,1))

```

