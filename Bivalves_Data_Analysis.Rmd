---
title: "Bivalves_Analysis"
author: "Dan Bez Golanski"
date: "`r Sys.Date()`"
output: html_document
---

##Functions

```{r}
# The function takes datetime data that was saved as text in csv/xlsx file and convert it to POSIxlt, can handle combined data including both numeric and non numeric datetime
Convert_time_csv <- function(data, final_format, initial_format="ymd HMS",correct_time=T ) {
  temp_data<-data
  #for data saved as numeric in csv
  temp_data<-as.numeric(temp_data)
  temp_data<-as_datetime(as_date(temp_data, origin = "1899-12-30"), tz = Sys.timezone())
  #for data not saved as numeric in csv
  na_index<-which(is.na(temp_data))
  if(length(na_index)!=length(data))
  {
    for (i in na_index) {
      temp_data[i]<-parse_date_time(data[i],orders = initial_format,truncated = 3)
    }
  } else # if all data is character
  {
    temp_data<-parse_date_time(data,orders = initial_format,truncated = 3)
  }
  temp_data <- as.POSIXlt(temp_data)
  if(correct_time)
  {
    #correct time that rounded wrongly because it was numeric
    for (row_num in 1:length(temp_data)) {
    temptime<-unclass(temp_data[row_num])
    tempsec<-temptime$sec
    if (tempsec>58) {
      temp_data[row_num]<-temp_data[row_num]+1
      }
    }
  }
  temp_data<-strftime(temp_data,format = final_format)
  return(temp_data)
}

# This functions gets a bivalve df and returns it after calculate the different metrics for a given time interval
df_cut_times <- function(biv_df,date_col_num,break_interval,date_format) {
  biv_df[,date_col_num] <- parse_date_time(biv_df[,date_col_num],orders = date_format)
  biv_df$group <- cut(biv_df[,date_col_num], breaks= break_interval)
  biv_df_cut_time <-biv_df %>%
  group_by(Deployment.number,Biv_num,group) %>%
  summarise(biv_open_interval_perc=mean(biv_open_percent),biv_open_interval=mean(biv_open),per99.9=mean(percentile.99))
  return(biv_df_cut_time)
}
```

##packages

```{r}
library(dplyr)
library(ggplot2)
library(lubridate)
library(openxlsx)
library(tidyr)
library(GGally)
library(scales)
library(Hmisc)
library(StreamMetabolism)
library(mgcv)

```

##load data

```{r}

db<-readRDS("Big_Hobo_database_raw.RDS")
db2wide<-read.csv("Big_Hobo_database_wide_2min_percent.csv")
dblong<-read.csv("Big_Hobo_databse_long_raw.csv")
dblong2<-read.csv("Big_Hobo_databse_long_2min_percent.csv")
dblong10<-read.csv("Big_Hobo_databse_long_10min_percent&env.csv")
dblong_hour <- read.csv("Big_Hobo_databse_long_hour_percent&env.csv")
```

##Change raw data to long format

```{r}
col_names<-c("Bivalve.4","Bivalve.6","Bivalve.7","Bivalve.8","Bivalve.0","Bivalve.12","Bivalve.21...","Bivalve.19","Bivavle.15","Bivavle.18","Bivavle.23","Bivavle.22")
dblong<-gather_(db,"Biv_num","biv_open",col_names,factor_key = T,na.rm = T)
dblong$Date...Time <- parse_date_time(dblong$Date...Time,orders = "dmy HMS",truncated = 3)
write.csv(dblong,"Big_Hobo_databse_long_raw.csv")
```

##Calculate opening percentages

```{r}
max_deploy<-max(dblong$Deployment.number)
for (dep_num in 1:max_deploy) {
  temp_array_deploy<-dblong[dblong$Deployment.number==dep_num,]
  biv_num<-unique(temp_array_deploy$Biv_num)
  for (biv_index in 1:length(biv_num)) {
    temp_array_bivnum <- temp_array_deploy %>%  filter(Biv_num==biv_num[biv_index])
    per99<-quantile(temp_array_bivnum$biv_open,0.999)
    dblong[dblong$Deployment.number==dep_num&dblong$Biv_num==biv_num[biv_index],5]<-per99
    names(dblong)[5]<-"percentile.99"
  }
}
rm(temp_array_bivnum,temp_array_deploy)
dblong$biv_open_percent<-dblong$biv_open/dblong$percentile.99*100
write.csv(dblong,"Big_Hobo_databse_long_raw_percent.csv")
```

##Average by different time intervals

```{r}
dblong2 <- df_cut_times(dblong,1,"2 min","ymd HMS")
dblong10 <- df_cut_times(dblong,1,"10 min","ymd HMS")
dblong_hour <- df_cut_times(dblong,1,"1 hour","ymd HMS")
dblong_week <- df_cut_times(dblong,1,"1 week","ymd HMS")
write.csv(dblong2, "Big_Hobo_databse_long_2min_percent.csv")
write.csv(dblong10, "Big_Hobo_databse_long_10min_percent.csv")
write.csv(dblong_hour, "Big_Hobo_databse_long_hour_percent.csv")
write.csv(dblong_week, "Big_Hobo_databse_long_week_percent.csv")

```

## Env_10min

```{r}
mx<-readRDS("BigMx.RDS")
obs<-readRDS("BigOBS.RDS")
#mx
names(mx)<-c("group","Temp","lux")
mx$group <- Convert_time_csv(mx$group,"%Y-%m-%d %H:%M:%S")
write.xlsx(mx,"BigMx.xlsx")
#merge to the bigHoboDatabase
dblong10<-merge(dblong10,mx,by="group",all.x = T)

#obs
names(obs)<-c("group","Turbidity")
obs$group <- Convert_time_csv(obs$group,"%Y-%m-%d %H:%M:%S")
write.xlsx(obs,"BigOBS.xlsx")
#merge to the bigHoboDatabase
dblong10<-merge(dblong10,obs,by="group",all.x = T)

#change NA to none
dblong10<-arrange(dblong10,Deployment.number,Biv_num,group)
dblong10 <- replace(dblong10, is.na(dblong10), "")
write.csv(dblong10, "Big_Hobo_databse_long_10min_percent&env.csv")
```

## Env_1hour and add obs and mx 1 hour average

```{r}
waves<-readRDS("Waves_data.RDS")
currents<-readRDS("Currents_data.RDS")
# waves
names(waves)[1]<-"group"
waves$group <- Convert_time_csv(waves$group,final_format = "%Y-%m-%d %H:%M:%S",initial_format = "dmy HM",correct_time = F)
dblong_hour<-merge(dblong_hour,waves,by="group",all.x = T)
rm(waves)

#currents
#get only bottom currents
currents<-currents %>% filter(Depth..m.==23)
currents<-currents[,-2]
names(currents)[1]<-"group"
currents$group <- Convert_time_csv(currents$group,"%Y-%m-%d %H:%M:%S","dmy HM",correct_time = F)
dblong_hour<-merge(dblong_hour,currents,by="group",all.x = T)
rm(currents)

#change NA to none
names(dblong_hour)<-c("Date_time","Deployment.number","Bivalve.number","mean.biv.gap.precent","mean.biv.open.degree","percentile.99","Hs[m]","Tp[s]","Dp[deg]","H1/10[m]","Tmean[s]","Current.Speed[cm/sec]","Current.Direction[deg]")
dblong_hour<-arrange(dblong_hour,Deployment.number,Bivalve.number,Date_time)
dblong_hour <- replace(dblong_hour, is.na(dblong_hour), "")

#average mx and obs and merge them
env_10min_to_1hour <- dblong10
#getting only environmental data and remove duplicates
env_10min_to_1hour <- env_10min_to_1hour %>% 
  select(Temp,lux,Turbidity,group) %>% 
  distinct()
env_10min_to_1hour$group <- parse_date_time(env_10min_to_1hour$group,orders = "ymd HMS")
env_10min_to_1hour$Date_time <- cut(env_10min_to_1hour$group, breaks= "1 hour")
env_10min_to_1hour$Temp <- as.numeric(env_10min_to_1hour$Temp)
env_10min_to_1hour$lux <- as.numeric(env_10min_to_1hour$lux)
env_10min_to_1hour$Turbidity <- as.numeric(env_10min_to_1hour$Turbidity)
env_10min_to_1hour <-env_10min_to_1hour %>%
  group_by(Date_time) %>%
  summarise(Temp_1hour_mean=mean(Temp),Lux_1hour_mean=mean(lux),Turbidity_1hour_mean=mean(Turbidity))
dblong_hour<-merge(dblong_hour,env_10min_to_1hour,by="Date_time",all.x = T)
rm(env_10min_to_1hour)
write.csv(dblong_hour, "Big_Hobo_databse_long_hour_percent&env.csv")
```

## Env_1week 
```{r}
reco<-read.csv("RECO_Database - Data.csv")
reco$mm.dd.yyyy<-parse_date_time(reco$mm.dd.yyyy,orders = "mdy",truncated = 3)
#divide into chucks of weeks
reco$group <- cut(reco$mm.dd.yyyy, breaks="week")
#filter to only shallow station 
reco <- reco %>%filter(Station=="ST3") %>%
  select(group,Cal.Chl...ug.L.,Depth..m.) %>% 
  group_by(group)
reco <- na.omit(reco) 
#summarise to all depths
reco <- reco %>% 
  summarise(Chl=mean(Cal.Chl...ug.L.)) %>% 
  arrange(group)
#make sure the group columns are the same
reco$group <- as.POSIXct(reco$group,format = "%Y-%m-%d")
dblong_week$group <- as.POSIXct(dblong_week$group,format = "%Y-%m-%d")
dblong_week <- merge(dblong_week,reco,by="group",all.x = T)

```

##2min sunrise.set and Day/Night calculations
```{r}
##Sunrise.set calculations

#sampling point coordinates
rei_point_crds <- c(32.4015190,34.8579670)
dblong2$group<- parse_date_time(dblong2$group,orders = "ymd HMS")
#creating column represent only the sampling dates
dblong2$Date <- as.Date(dblong2$group)
sampling_dates <- unique(dblong2$Date)
#create a data frame for sunrise.set for all the sampling dates
temp_date <- data.frame(matrix(NA,ncol=3,nrow=length(sampling_dates)))
names(temp_date) <- c("Date","Sunrise","Sunset")
temp_date$Date <- sampling_dates
rm(sampling_dates)
#calculate sunrise.set
for (day in 1:length(temp_date$Date)) {
  sunrise_set <- sunrise.set(rei_point_crds[1],rei_point_crds[2], as.Date(temp_date$Date[day]),timezone = "Asia/Jerusalem")
  temp_date$Sunrise[day] <- as.POSIXct(sunrise_set$sunrise)
  temp_date$Sunset[day] <- as.POSIXct(sunrise_set$sunset)
}
#change from numeric to datetime
temp_date$Sunrise <- as.POSIXct(as.numeric(temp_date$Sunrise),origin='1970-01-01')
temp_date$Sunset <- as.POSIXct(as.numeric(temp_date$Sunset),origin='1970-01-01')
temp_date$Sunrise <- force_tz(temp_date$Sunrise,tzone = "UTC") 
temp_date$Sunset <- force_tz(temp_date$Sunset,tzone = "UTC") 

dblong2<-merge(dblong2,temp_date,by="Date",all.x = T)
dblong2$Date <- NULL
rm(temp_date)


##Day.Night Calculations
dblong2$Day_Night <- ifelse(dblong2$group>dblong2$Sunrise & dblong2$group<dblong2$Sunset,"Day","Night")
#create new data frame composing only from distinct date times
temp_sun <- dblong2 %>% select(group,Sunrise,Sunset,Day_Night,Deployment.number) %>% distinct()
#calculate time difference from sunset for the day times
temp_sun$diff_sunset[temp_sun$Day_Night=="Day"] <- difftime(temp_sun$group[temp_sun$Day_Night=="Day"],temp_sun$Sunset[temp_sun$Day_Night=="Day"],units = "hours")
#calculate time difference for night times, complicated because the change of the dates at midnight
temp_night <- temp_sun[temp_sun$Day_Night=="Night",]
temp_night <- temp_night %>% arrange(group)
#calculate for times before midnight
temp_night$diff_sunset[as.numeric(format(temp_night$group,"%H"))>10] <-difftime(temp_night$group[ as.numeric(format(temp_night$group,"%H"))>10],temp_night$Sunset[as.numeric(format(temp_night$group ,"%H"))>10],units = "hours")

#calculate time difference for night times after midnight
dep_num <- 0
for (row_num in 1:dim(temp_night)[1]) {
  if (!is.na(temp_night$diff_sunset[row_num])) {
    #save the sunset time of the date before midnight
    temp_sunset <- temp_night$Sunset[row_num]
    #save the deployment number to avoid deployments mixing
    dep_num <- temp_night$Deployment.number[row_num]
  } else if (is.na(temp_night$diff_sunset[row_num]) & temp_night$Deployment.number[row_num]==dep_num) {
    temp_night$diff_sunset[row_num] <- difftime(temp_night$group[row_num],temp_sunset)
  }
}
#merging all the  data frames together
temp_night <- temp_night %>% select(group,diff_sunset)
#merging two partly full columns into one
temp_sun <- temp_sun %>% left_join(y,by="group") %>%
  mutate(diff_sunset = ifelse(is.na(diff_sunset.x),diff_sunset.y,diff_sunset.x)) %>% 
  select(-diff_sunset.x,-diff_sunset.y)
temp_sun <- temp_sun %>% select(group,diff_sunset) %>% arrange(group)
dblong2 <- merge(dblong2,temp_sun,by = "group",all.x = T)
rm(temp_sun)
rm(temp_night)
saveRDS(dblong2,"Big_Hobo_database_long_2min_percent&sun.RDS")
```


##Bivalves correlations
```{r}
#Bivalves correlations
dbwide2 <- dblong2 %>% select(group,Deployment.number,Biv_num,biv_open_interval_perc) %>% 
  spread(key = Biv_num,value = biv_open_interval_perc)
write.csv(dbwide2,"Big_Hobo_database_wide_2min_percent.csv")
deployments <- unique(dbwide2$Deployment.number)
bivalve_corr <- data.frame(dep_num=integer(),
                           corr_values=double(),
                           p_values=double(),
                           n=integer()
)
for (dep in deployments) {
temp_dep <- dbwide2 %>% filter(Deployment.number==dep)
#remove bivalves weren't deployed in the specific deployment
temp_dep <- temp_dep[,colSums(!is.na(temp_dep))>0]
# remove datetime and deployment number
temp_dep <- temp_dep[-1:-2]
#getting correlation, p-values and n and convert them to data frame
temp_corr <- rcorr(as.matrix(temp_dep),type = "pearson")
temp_corr <-data.frame(dep_num = paste("Deployment",dep),
                       corr_values = as.vector(temp_corr[["r"]][upper.tri(temp_corr[["r"]])]),
                       p_values = as.vector(temp_corr[["P"]][upper.tri(temp_corr[["P"]])]),
                       n = as.vector(temp_corr[["n"]][upper.tri(temp_corr[["n"]])])
                       )
bivalve_corr <- rbind(bivalve_corr,temp_corr)                       
}
rm(temp_corr,temp_dep)
write.csv(bivalve_corr,"Bivalves_correlations.csv")
hist(bivalve_corr$corr_values)

```



##Environmental correlations
```{r}
#Environmental correlations - hourly
#wide bivalves
dbwide_hour <- dblong_hour %>% select(Date_time,Deployment.number,Bivalve.number,mean.biv.gap.precent) %>%
  spread(key = Bivalve.number,value = mean.biv.gap.precent)
#wide environment
environment_wide <- dblong_hour %>% select(Date_time,Deployment.number,Hs.m.,Tp.s.,Dp.deg.,H1.10.m.,Tmean.s.,Current.Speed.cm.sec.,Current.Direction.deg.,Temp_1hour_mean,Lux_1hour_mean,Turbidity_1hour_mean) %>% 
  distinct()
#get all the deployments
deployments <- unique(dbwide_hour$Deployment.number)
#prepare the big data frame
env_corr <- data.frame(dep_num=integer(),
                       biv_num=character(),
                       environmental_factor=character(),
                       corr_values=double(),
                       p_values=double(),
                       n=integer()
)

for (dep in deployments) {
temp_dep <- dbwide_hour %>% filter(Deployment.number==dep)
#remove bivalves weren't deployed in the specific deployment
temp_dep <- temp_dep[,colSums(!is.na(temp_dep))>0]
# get the bivalves names
temp_bivalves_id <- names(temp_dep)[3:dim(temp_dep)[2]]
#filter environmental data to a specific deployment
temp_env <- environment_wide %>% filter(Deployment.number==dep)
temp_env <- temp_env %>% select(-Deployment.number)
for (biv_num in temp_bivalves_id) {
  temp_biv <- temp_dep[,c(1,which(colnames(temp_dep)==biv_num ))]
  temp_biv <- merge(temp_biv,temp_env,by="Date_time",all.x=T)
  #remove datetime
  temp_biv <- temp_biv[-1]
  #getting correlation, p-values and n and convert them to data frame
  temp_corr <- rcorr(as.matrix(temp_biv),type = "pearson")
  temp_corr <-data.frame(dep_num = paste("Deployment",dep),
                       biv_num=biv_num,
                       #starting from 2 to avoid autocorrelation with the bivalve
                       #getting the environmental factors names
                       environmental_factor=colnames(temp_corr[["r"]])[2:length(colnames(temp_corr[["r"]]))],
                       corr_values = as.vector(temp_corr[["r"]][1,2:dim(temp_corr[["r"]])[2]]),
                       p_values = as.vector(temp_corr[["P"]][1,2:dim(temp_corr[["P"]])[2]]),
                       n = as.vector(temp_corr[["n"]][1,2:dim(temp_corr[["n"]])[2]])
                       )
env_corr <- rbind(env_corr,temp_corr)
  }
}
env_corr <- na.omit(env_corr)
rm(temp_corr,temp_dep)
write.csv(env_corr,"Environmental_correlations.csv")
hist(env_corr$corr_values)



##Environmental correlations - weekly (chl) - poor reco 
#wide bivalves
dbwide_week <- dblong_week %>% select(group,Deployment.number,Biv_num,biv_open_interval_perc) %>%
  spread(key = Biv_num,value = biv_open_interval_perc)
#wide environment
environment_week_wide <- dblong_week %>% select(group,Deployment.number,Chl) %>% 
  distinct()
#get all the deployments
deployments <- unique(dbwide_week$Deployment.number)
#prepare the big data frame
env_corr_week <- data.frame(dep_num=integer(),
                       biv_num=character(),
                       environmental_factor=character(),
                       corr_values=double(),
                       p_values=double(),
                       n=integer()
)

for (dep in deployments) {
temp_dep <- dbwide_week %>% filter(Deployment.number==dep)
#remove bivalves weren't deployed in the specific deployment
temp_dep <- temp_dep[,colSums(!is.na(temp_dep))>0]
# get the bivalves names
temp_bivalves_id <- names(temp_dep)[3:dim(temp_dep)[2]]
#filter environmental data to a specific deployment
temp_env <- environment_wide %>% filter(Deployment.number==dep)
temp_env <- temp_env %>% select(-Deployment.number)
if(length(which(!is.na(temp_env$Chl)))>3)
{
  for (biv_num in temp_bivalves_id) {
    temp_biv <- temp_dep[,c(1,which(colnames(temp_dep)==biv_num ))]
    temp_biv <- merge(temp_biv,temp_env,by="group",all.x=T)
    #remove datetime
    temp_biv <- temp_biv[-1]
    #getting correlation, p-values and n and convert them to data frame
    temp_corr <- rcorr(as.matrix(temp_biv),type = "pearson")
    temp_corr <-data.frame(dep_num = paste("Deployment",dep),
                       biv_num=biv_num,
                       #starting from 2 to avoid autocorrelation with the bivalve
                       #getting the environmental factors names
                       environmental_factor="Chl",
                       corr_values = as.vector(temp_corr[["r"]][1,2:dim(temp_corr[["r"]])[2]]),
                       p_values = as.vector(temp_corr[["P"]][1,2:dim(temp_corr[["P"]])[2]]),
                       n = as.vector(temp_corr[["n"]][1,2:dim(temp_corr[["n"]])[2]])
                       )
    env_corr_week <- rbind(env_corr_week,temp_corr)
    }
  }
}
env_corr_week <- na.omit(env_corr_week)
rm(temp_corr,temp_dep)
write.csv(env_corr_week,"Chl_correlations_week.csv")
hist(env_corr_week$corr_values)




```



##Bivalve opening times(how much time was open in whole duration or daily)
```{r}
###check mean and max opening time for each bivalve for the whole deployments duration
bivalves_names<-unique(dblong2$Biv_num)
#prepare the dataframe
bivalves_opening_times<-data.frame(
  bivalve_name=character(),
  mean.opening.time=double(),
  max.opening.time=double(),
  n=double(),
  sd=double(),
  se=double(),
  limit=double()
)
#Check different limits defined as closed
precent_limits<-c(1,5,10,15,20,30)
#loop for all limits
#index for which precent limit iteration are we in
limit_iter_num <- 0 
for (limit in precent_limits) {
  #loop for all bivalves
  for (biv_num in 1:length(bivalves_names)) {
    #filter by bivalve
    temp_biv<-filter(dblong2,dblong2$Biv_num==bivalves_names[biv_num])
    open_events<-c()
    bivalves_opening_times[biv_num+limit_iter_num,]<-NA
    bivalves_opening_times$bivalve_name[biv_num+limit_iter_num]<-bivalves_names[biv_num]
    bivalves_opening_times$limit[biv_num+limit_iter_num]<-limit
    open_index<-1
    total_time<-0#for computing  total time of measurement per bivalve
    sum_time_open<-0
    #loop for each bivalve
    for (row in 1:dim(temp_biv)[1]) {
      total_time<-total_time+2
      #check if the bivalve closed or if we passed deployment or passed day
      if(temp_biv$biv_open_interval_perc[row]<limit || (row!=1 && temp_biv$Deployment.number[row]!=temp_biv$Deployment.number[row-1]))
      {
        #check if it was open before
        if(sum_time_open>0)
        {
          #initializing the next open event
          open_events[open_index]<-NA
          #add the opening time to the series of all the gaping events
          open_events[open_index]<-sum_time_open
          #increase the number of gaping events
          open_index<-open_index+1
        }
        #zeroing the counter of the time the bivalve is open 
        sum_time_open<-0
      } 
      else
      {
        #increase the time of gaping by 2 because each row represent 2 minutes
        sum_time_open<-sum_time_open+2
      }
    } 
    #maximum time open continously during 1 deployment
    bivalves_opening_times$max.opening.time[biv_num+limit_iter_num]<-max(open_events)
    #mean time open continously during 1 deployment
    bivalves_opening_times$mean.opening.time[biv_num+limit_iter_num]<-mean(open_events) 
    #amount of distinct opening events
    bivalves_opening_times$n[biv_num+limit_iter_num]<-length(open_events)
    bivalves_opening_times$sd[biv_num+limit_iter_num]<-sd(open_events)
    bivalves_opening_times$se[biv_num+limit_iter_num]<-bivalves_opening_times$sd[biv_num+limit_iter_num]/sqrt(bivalves_opening_times$n[biv_num+limit_iter_num])
    #precent of time open in compare to the whole deployment duration of the bivalve
    bivalves_opening_times$total_time_open[biv_num+limit_iter_num]<-sum(open_events)/total_time*100
    #for computing total time open percetange for each bivalve
  }
  limit_iter_num<-limit_iter_num+12
}
write.csv(bivalves_opening_times,"Bivalves_open_all_duration.csv")

##check mean and max opening time for each bivalve for the each day
bivalves_daily_open <- data.frame(
  bivalve_name=character(),
  date=Date(),
  open_time_ratio=double(),
  n=integer(),
  limit=integer()
)
#loop for all bivalve
limit_iter_num<-0 #index for precent limits
for (limit in precent_limits) {
  for (biv_num in 1:length(bivalves_names)) {
    #filter by bivalve
    temp_biv<-dplyr::filter(dblong2,dblong2$Biv_num==bivalves_names[biv_num])
    temp_biv$date<-as.Date(temp_biv$group)
    dates<-unique(temp_biv$date)
    open_times<-c(NA,length(dates))
    #how much time of the day was covered
    data_in_day <- c(NA,length(dates))
    #loop for all dates
    for (date_num in 1:length(dates)) {
      temp_date<-dplyr::filter(temp_biv,temp_biv$date==dates[date_num])
      sum_time<-0
      #loop for each day
      for (row in 1:dim(temp_date)[1]) {
        if(temp_date$biv_open_interval_perc[row]>limit ){
          sum_time<-sum_time+2
        } 
      }
      open_times[date_num]<-sum_time/(24*60)*100
      data_in_day[date_num] <- dim(temp_date)[1]
    }
    temp_daily_open <- data.frame(
      bivalve_name=bivalves_names[biv_num],
      date=dates,
      open_time_ratio=open_times,
      n=data_in_day,
      limit=limit
    )
    bivalves_daily_open <- rbind(bivalves_daily_open,temp_daily_open)
  }
  limit_iter_num<-limit_iter_num+12
  write.csv(bivalves_daily_open,"Bivalves_daily_open.csv")
}
```

## Including Plots

```{r}
#seasonal and daily trend along bivalves - Work!
dblong2$group <- parse_date_time(dblong2$group,orders = "ymd HMS")
dblong2$Month <- format(dblong2$group,"%Y-%m")
dblong2$TimeofDay <- as.POSIXct(hms::as_hms(dblong2$group)) 
months_gaping_graph <- ggplot(dblong2, aes(x = TimeofDay, y = biv_open_interval_perc))+
  geom_point(color = "grey", size = 0.1)+
  geom_smooth(linewidth = 2, color = "blue",se = T,fill="red")+
  # geom_spline(spar = 0.05, linewidth = 2, color = "blue")+
  scale_x_datetime( date_breaks = "4 hours",date_labels = "%H:%M") +
  scale_y_continuous(limits = c(0,100))+
  facet_wrap(~Month, scales = "free_x", ncol = length(unique(dblong2$Month))) +
  labs(x = "Time of Day",
       y = "Gaping Percent") +
  theme_bw()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"),
        strip.background = element_rect(fill="white",color="black", linewidth = 1.5, linetype="solid"),
        axis.title.y = element_text(size = 20),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 14),
        axis.text.x = element_text(size = 14, angle = 45, hjust = 1),
        strip.text = element_text(size = 14),
        legend.text = element_text(size = 14))
  months_gaping_graph
  
#Day Night difference -  Work only for 0 to 100 range!
  Day_Night_gaping_graph <- ggplot(dblong2, aes(x = diff_sunset, y = biv_open_interval_perc)) +
    geom_point(size = 0.1, alpha = 0.5, color = "gray") +
    geom_smooth(data = subset(dblong2,diff_sunset>0), size = 1.5,color = "blue", se =T,fill="yellow")+ 
    geom_smooth(data = subset(dblong2,diff_sunset<0), size = 1.5,color = "red",se =T,fill="yellow")+
    geom_vline(xintercept = 0,linetype="dashed",color="black")+
    scale_y_continuous(limits = c(0,100
                                  ))+
    scale_x_continuous(limits = c(-12,12),breaks = seq(-12,12,2))+
    labs(title = "Bivalve Open Interval Percentage Over Time",
         x = "Time",
         y = "Bivalve Open Percentage") 
Day_Night_gaping_graph

#bivalve correlation graph - not on top of each other and not beautiful!
correlation_hitsogram <- ggplot(bivalve_corr, aes(corr_values)) +
  geom_histogram(binwidth = 0.1,breaks = seq(-1, 1, by = 0.1), fill = "lightblue", color = "black") +
  scale_x_continuous(limits = c(-1,1))+
  labs(title = "Distribution of corr_values with Box Plot",
       x = "Correlation values",
       y = "Count") 

correlation_boxplot <- ggplot(bivalve_corr, aes(corr_values))+geom_boxplot(aes(y=corr_values,x=NULL))+
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", size = 1) +coord_flip()

layout(mat = matrix(c(1,2),2,1, byrow=TRUE),  height = c(1,8))
 
# Draw the boxplot and the histogram 
par(mar=c(0, 3.1, 1.1, 2.1))
boxplot(bivalve_corr$corr_values , horizontal=TRUE , ylim=c(-1,1), xaxt="n" , col=rgb(0.8,0.8,0,0.5) , frame=F)
par(mar=c(4, 3.1, 1.1, 2.1))
hist(bivalve_corr$corr_values , breaks=20 , col=rgb(0.2,0.8,0.5,0.5) , border=F , main="" , xlab="Bivalves correlations", xlim=c(-1,1),ylim = c(0,30))

```

